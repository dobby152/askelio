#!/usr/bin/env python3
"""
üöÄ ASKELIO DOCUMENT PROCESSING API v3.0 - CLEAN ARCHITECTURE
Ultra Cost-Effective API with Powerful LLM Models (Claude 3.5 Sonnet, GPT-4o)

CLEAN IMPLEMENTATION:
- Only unified_document_processor (no legacy components)
- Powerful models with deep understanding
- Minimal, focused endpoints
- Production-ready architecture
"""

from fastapi import FastAPI, HTTPException, UploadFile, File, Depends
from fastapi import Form
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
import os
import tempfile
import time
import logging
from datetime import datetime
from dotenv import load_dotenv
from sqlalchemy.orm import Session

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

from database_sqlite import get_db, init_db
from models_sqlite import Document, ExtractedField
from unified_document_processor import UnifiedDocumentProcessor, ProcessingOptions, ProcessingMode

# Load environment variables
load_dotenv()

# üöÄ Initialize ONLY Unified Document Processor (Clean Architecture)
unified_processor = UnifiedDocumentProcessor()

# Initialize SQLite database
init_db()

# FastAPI app
app = FastAPI(
    title="Askelio Document Processing API v3.0",
    description="üöÄ Clean Architecture with Powerful LLM Models",
    version="3.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request/Response logging middleware
@app.middleware("http")
async def log_requests(request, call_next):
    start_time = time.time()
    logger.info(f"üîç Request: {request.method} {request.url}")
    logger.info(f"üîç Headers: {dict(request.headers)}")
    
    response = await call_next(request)
    
    process_time = time.time() - start_time
    logger.info(f"‚úÖ Response: {response.status_code} in {process_time:.3f}s")
    
    return response

@app.get("/")
async def root():
    return {
        "message": "Askelio Document Processing API v3.0",
        "description": "üöÄ Clean Architecture with Powerful LLM Models (Claude 3.5 Sonnet, GPT-4o)",
        "endpoints": {
            "POST /api/v1/documents/process": "üéØ MAIN: Unified document processing",
            "GET /health": "Health check",
            "GET /api/v1/system/status": "System status",
            "GET /documents": "List processed documents"
        },
        "features": [
            "Claude 3.5 Sonnet (Flagship)",
            "GPT-4o (Premium)", 
            "Claude 3 Haiku (Optimal)",
            "Deep context understanding",
            "Czech language support",
            "Cost-effective processing"
        ]
    }

@app.get("/health")
async def health_check():
    return {"status": "healthy", "version": "3.0.0", "architecture": "clean"}

# üéØ MAIN ENDPOINT - Unified Document Processing
@app.post("/api/v1/documents/process")
async def process_document_unified(
    file: UploadFile = File(...),
    mode: str = "cost_optimized",
    max_cost_czk: float = 5.0,
    min_confidence: float = 0.8,
    enable_fallbacks: bool = True,
    return_raw_text: bool = False,
    enable_ares_enrichment: bool = True
):
    """
    üéØ UNIFIED DOCUMENT PROCESSING ENDPOINT
    
    Powerful models with deep understanding and context awareness.
    
    Parameters:
    - mode: cost_optimized (default), accuracy_first, speed_first, budget_strict
    - max_cost_czk: Maximum cost per document in CZK (default: 5.0 for powerful models)
    - min_confidence: Minimum acceptable confidence (default: 0.8)
    - enable_fallbacks: Enable fallback providers (default: true)
    - return_raw_text: Include raw OCR text in response (default: false)
    - enable_ares_enrichment: Enable ARES company data enrichment (default: true)
    
    Returns consistent format with powerful model results.
    """

    if not file.filename:
        return {
            "success": False,
            "data": None,
            "meta": {"processing_time": 0.0, "cost_czk": 0.0},
            "error": {"code": "NO_FILE", "message": "No file provided"}
        }

    # Validate file type
    allowed_types = [
        "application/pdf",
        "image/jpeg", "image/jpg", "image/png",
        "image/gif", "image/bmp", "image/tiff"
    ]

    if file.content_type not in allowed_types:
        return {
            "success": False,
            "data": None,
            "meta": {"processing_time": 0.0, "cost_czk": 0.0},
            "error": {
                "code": "UNSUPPORTED_FILE_TYPE",
                "message": f"Unsupported file type: {file.content_type}",
                "supported_types": allowed_types
            }
        }

    # Validate file size (10MB limit)
    if file.size and file.size > 10 * 1024 * 1024:
        return {
            "success": False,
            "data": None,
            "meta": {"processing_time": 0.0, "cost_czk": 0.0},
            "error": {
                "code": "FILE_TOO_LARGE",
                "message": "File too large (max 10MB)",
                "file_size_mb": round(file.size / (1024*1024), 2)
            }
        }

    # Save file temporarily
    with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.filename)[1]) as temp_file:
        content = await file.read()
        temp_file.write(content)
        temp_file_path = temp_file.name

    try:
        # Parse processing mode
        try:
            processing_mode = ProcessingMode(mode)
        except ValueError:
            processing_mode = ProcessingMode.COST_OPTIMIZED

        # Create processing options
        options = ProcessingOptions(
            mode=processing_mode,
            max_cost_czk=max_cost_czk,
            min_confidence=min_confidence,
            enable_fallbacks=enable_fallbacks,
            store_in_db=True,
            return_raw_text=return_raw_text,
            enable_ares_enrichment=enable_ares_enrichment
        )

        # Process document with unified processor
        result = unified_processor.process_document(temp_file_path, file.filename, options)

        # Clean up temp file
        os.unlink(temp_file_path)

        # Build consistent response
        if result.success:
            response_data = {
                "document_id": result.document_id,
                "document_type": result.document_type.value,
                "structured_data": result.structured_data,
                "confidence": result.confidence
            }

            if return_raw_text:
                response_data["raw_text"] = result.raw_text

            return {
                "success": True,
                "data": response_data,
                "meta": {
                    "processing_time": result.processing_time,
                    "cost_czk": result.cost_czk,
                    "provider_used": result.provider_used,
                    "fallbacks_used": result.fallbacks_used,
                    "validation_notes": result.validation_notes,
                    "raw_google_vision_text": result.raw_text  # üîç DEBUG: Raw Google Vision text
                },
                "error": None
            }
        else:
            return {
                "success": False,
                "data": None,
                "meta": {
                    "processing_time": result.processing_time,
                    "cost_czk": result.cost_czk,
                    "provider_used": result.provider_used,
                    "fallbacks_used": result.fallbacks_used
                },
                "error": {
                    "code": "PROCESSING_FAILED",
                    "message": result.error_message or "Document processing failed"
                }
            }

    except Exception as e:
        # Clean up temp file on error
        if os.path.exists(temp_file_path):
            os.unlink(temp_file_path)

        logger.error(f"‚ùå Unified processing error: {e}")
        return {
            "success": False,
            "data": None,
            "meta": {"processing_time": 0.0, "cost_czk": 0.0},
            "error": {
                "code": "INTERNAL_ERROR",
                "message": f"Internal processing error: {str(e)}"
            }
        }

# üìä SYSTEM STATUS
@app.get("/api/v1/system/status")
async def get_system_status():
    """Get comprehensive system status"""
    try:
        stats = unified_processor.get_statistics()
        return {
            "status": "success",
            "system_ready": True,
            "version": "3.0.0",
            "architecture": "clean_unified_processor",
            "statistics": stats,
            "powerful_models": {
                "flagship": "Claude 3.5 Sonnet",
                "premium": "GPT-4o", 
                "optimal": "Claude 3 Haiku",
                "budget": "GPT-4o Mini"
            }
        }
    except Exception as e:
        logger.error(f"‚ùå System status error: {e}")
        return {
            "status": "error",
            "system_ready": False,
            "error": str(e)
        }

# üìÑ DOCUMENT MANAGEMENT
@app.get("/documents")
async def get_documents(db: Session = Depends(get_db)):
    """Get list of processed documents"""
    documents = db.query(Document).order_by(Document.created_at.desc()).all()
    return [
        {
            "id": doc.id,
            "filename": doc.filename,
            "status": doc.status,
            "confidence": doc.confidence,
            "processing_time": doc.processing_time,
            "cost_czk": getattr(doc, 'cost_czk', 0.0),
            "provider_used": doc.provider_used,
            "created_at": doc.created_at.isoformat() if doc.created_at else None
        }
        for doc in documents
    ]

@app.get("/documents/{document_id}")
async def get_document(document_id: int, db: Session = Depends(get_db)):
    """Get specific document details"""
    document = db.query(Document).filter(Document.id == document_id).first()
    if not document:
        raise HTTPException(status_code=404, detail="Document not found")
    
    # Get extracted fields
    fields = db.query(ExtractedField).filter(ExtractedField.document_id == document_id).all()
    
    return {
        "id": document.id,
        "filename": document.filename,
        "status": document.status,
        "confidence": document.confidence,
        "processing_time": document.processing_time,
        "provider_used": document.provider_used,
        "extracted_text": document.extracted_text,
        "ares_enriched": document.ares_enriched,  # Include ARES metadata
        "created_at": document.created_at.isoformat() if document.created_at else None,
        "extracted_fields": [
            {
                "field_name": field.field_name,
                "field_value": field.field_value,
                "confidence": field.confidence,
                "data_type": field.data_type
            }
            for field in fields
        ]
    }

@app.delete("/documents/{document_id}")
async def delete_document(document_id: int, db: Session = Depends(get_db)):
    """Delete a document and all its associated data"""
    # Get document from database
    document = db.query(Document).filter(Document.id == document_id).first()
    if not document:
        raise HTTPException(status_code=404, detail="Document not found")

    try:
        # Delete associated extracted fields first (foreign key constraint)
        db.query(ExtractedField).filter(ExtractedField.document_id == document_id).delete()

        # Delete the document
        db.delete(document)
        db.commit()

        return {
            "success": True,
            "message": f"Document '{document.filename}' deleted successfully",
            "deleted_document_id": document_id
        }
    except Exception as e:
        db.rollback()
        raise HTTPException(status_code=500, detail=f"Failed to delete document: {str(e)}")

@app.get("/documents/{document_id}/export")
async def export_document(
    document_id: int,
    format: str = "json",
    include_ares: bool = True,
    db: Session = Depends(get_db)
):
    """
    Export document with ARES enriched data
    Supports JSON, CSV, and XML formats with full ARES integration
    """
    document = db.query(Document).filter(Document.id == document_id).first()
    if not document:
        raise HTTPException(status_code=404, detail="Document not found")

    # Get extracted fields and reconstruct structured data
    fields = db.query(ExtractedField).filter(ExtractedField.document_id == document_id).all()

    # Build structured data from fields
    structured_data = {}
    vendor_data = {}
    customer_data = {}

    for field in fields:
        if field.field_name.startswith("vendor."):
            key = field.field_name.replace("vendor.", "")
            vendor_data[key] = field.field_value
        elif field.field_name.startswith("customer."):
            key = field.field_name.replace("customer.", "")
            customer_data[key] = field.field_value
        else:
            structured_data[field.field_name] = field.field_value

    if vendor_data:
        structured_data["vendor"] = vendor_data
    if customer_data:
        structured_data["customer"] = customer_data

    # Include ARES metadata if available and requested
    if include_ares and document.ares_enriched:
        structured_data["_ares_enrichment"] = document.ares_enriched

    export_data = {
        "document_id": document.id,
        "filename": document.filename,
        "processed_at": document.processed_at.isoformat() if document.processed_at else None,
        "confidence": document.confidence,
        "provider_used": document.provider_used,
        "structured_data": structured_data,
        "export_metadata": {
            "exported_at": datetime.now().isoformat(),
            "format": format,
            "ares_included": include_ares and bool(document.ares_enriched)
        }
    }

    if format.lower() == "json":
        return export_data
    elif format.lower() == "csv":
        # Simple CSV export for structured data
        csv_lines = ["Field,Value,Confidence"]
        for field in fields:
            csv_lines.append(f'"{field.field_name}","{field.field_value}",{field.confidence or 0.0}')

        from fastapi.responses import Response
        return Response(
            content="\n".join(csv_lines),
            media_type="text/csv",
            headers={"Content-Disposition": f"attachment; filename=document_{document_id}.csv"}
        )
    else:
        raise HTTPException(status_code=400, detail="Unsupported format. Use 'json' or 'csv'")

@app.get("/api/v1/ares/test")
async def test_ares_integration():
    """
    Test ARES integration functionality
    Returns test results for known Czech companies
    """
    try:
        from ares_client import ares_client

        # Test zn√°m√Ωch IƒåO
        test_icos = [
            ("02445344", "Skanska Residential a.s."),
            ("27082440", "Alza.cz a.s.")
        ]

        results = []

        for ico, expected_name in test_icos:
            try:
                company_data = ares_client.get_company_data(ico)

                if company_data:
                    results.append({
                        "ico": ico,
                        "expected_name": expected_name,
                        "actual_name": company_data.name,
                        "dic": company_data.dic,
                        "address": company_data.address,
                        "is_active": company_data.is_active,
                        "is_vat_payer": company_data.is_vat_payer,
                        "success": True,
                        "name_match": expected_name.lower() in company_data.name.lower()
                    })
                else:
                    results.append({
                        "ico": ico,
                        "expected_name": expected_name,
                        "success": False,
                        "error": "Company not found"
                    })

            except Exception as e:
                results.append({
                    "ico": ico,
                    "expected_name": expected_name,
                    "success": False,
                    "error": str(e)
                })

        return {
            "status": "success",
            "ares_integration": "active",
            "test_results": results,
            "summary": {
                "total_tests": len(test_icos),
                "successful": len([r for r in results if r.get("success", False)]),
                "failed": len([r for r in results if not r.get("success", False)])
            }
        }

    except Exception as e:
        return {
            "status": "error",
            "ares_integration": "failed",
            "error": str(e)
        }

@app.get("/api/v1/ares/{ico}")
async def get_company_from_ares(ico: str):
    """
    Get company data from ARES by IƒåO
    """
    try:
        from ares_client import ares_client

        company_data = ares_client.get_company_data(ico)

        if company_data:
            return {
                "success": True,
                "ico": company_data.ico,
                "name": company_data.name,
                "dic": company_data.dic,
                "address": company_data.address,
                "legal_form": company_data.legal_form,
                "is_active": company_data.is_active,
                "is_vat_payer": company_data.is_vat_payer
            }
        else:
            raise HTTPException(status_code=404, detail="Company not found in ARES")

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ARES API error: {str(e)}")

# üìä DASHBOARD API ENDPOINTS

@app.get("/api/v1/dashboard/stats")
async def get_dashboard_stats():
    """
    Get dashboard statistics including financial metrics and trends
    """
    try:
        db = next(get_db())

        # Get all documents
        documents = db.query(Document).all()

        # Calculate financial metrics from documents
        total_amount = 0
        document_count = len(documents)

        for doc in documents:
            if doc.extracted_fields:
                for field in doc.extracted_fields:
                    if field.field_name == "total_amount" and field.value:
                        try:
                            amount = float(field.value.replace(',', '.').replace(' ', ''))
                            total_amount += amount
                        except (ValueError, AttributeError):
                            continue

        # Mock trends for now - in production these would be calculated from historical data
        stats = {
            "success": True,
            "data": {
                "totalIncome": total_amount * 1.2,  # Simulate income being higher than expenses
                "totalExpenses": total_amount,
                "netProfit": total_amount * 0.2,
                "remainingCredits": 1000,  # Mock value
                "processedDocuments": document_count,
                "trends": {
                    "income": 15.3,
                    "expenses": -8.7,
                    "profit": 23.8,
                    "credits": -5.2
                }
            },
            "meta": {
                "timestamp": datetime.now().isoformat(),
                "currency": "CZK"
            }
        }

        return stats

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get dashboard stats: {str(e)}")

@app.get("/api/v1/dashboard/recent-activity")
async def get_recent_activity():
    """
    Get recent activity for dashboard
    """
    try:
        db = next(get_db())

        # Get recent documents
        recent_docs = db.query(Document).order_by(Document.created_at.desc()).limit(5).all()

        activities = []
        for i, doc in enumerate(recent_docs):
            # Extract supplier name and amount
            supplier_name = "Nezn√°m√Ω dodavatel"
            amount = None

            if doc.extracted_fields:
                for field in doc.extracted_fields:
                    if field.field_name == "supplier_name" and field.value:
                        supplier_name = field.value
                    elif field.field_name == "total_amount" and field.value:
                        amount = field.value

            activities.append({
                "id": str(doc.id),
                "type": "invoice",
                "title": f"Nov√° faktura od {supplier_name}",
                "description": f"p≈ôed {i + 1} hodinami",
                "amount": f"{amount} CZK" if amount else None,
                "time": f"p≈ôed {i + 1} hodinami",
                "icon": "FileText",
                "color": "blue"
            })

        # Add some mock activities if we don't have enough real data
        if len(activities) < 3:
            activities.extend([
                {
                    "id": "approval-1",
                    "type": "approval",
                    "title": "Schv√°lena faktura #2024-001",
                    "description": "p≈ôed 4 hodinami",
                    "amount": "23,450 CZK",
                    "time": "p≈ôed 4 hodinami",
                    "icon": "CheckCircle",
                    "color": "green"
                },
                {
                    "id": "upload-1",
                    "type": "upload",
                    "title": "Nahr√°no 5 nov√Ωch dokument≈Ø",
                    "description": "vƒçera",
                    "time": "vƒçera",
                    "icon": "Upload",
                    "color": "purple"
                }
            ])

        return {
            "success": True,
            "data": activities[:5],  # Limit to 5 activities
            "meta": {
                "timestamp": datetime.now().isoformat()
            }
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get recent activity: {str(e)}")

@app.get("/api/v1/dashboard/ai-insights")
async def get_ai_insights():
    """
    Get AI-generated insights and recommendations for dashboard
    """
    try:
        db = next(get_db())

        # Get basic stats for generating insights
        documents = db.query(Document).all()
        document_count = len(documents)

        insights = []

        # Generate insights based on data
        if document_count > 0:
            insights.append({
                "type": "positive",
                "title": "Pozitivn√≠ trend",
                "description": "P≈ô√≠jmy rostou rychleji ne≈æ v√Ωdaje",
                "icon": "TrendingUp"
            })

        # Always add some insights
        insights.extend([
            {
                "type": "warning",
                "title": "Upozornƒõn√≠",
                "description": f"{min(3, document_count)} faktury s bl√≠≈æ√≠c√≠ se splatnost√≠" if document_count > 0 else "Zat√≠m nejsou nahr√°ny ≈æ√°dn√© dokumenty",
                "icon": "AlertTriangle"
            },
            {
                "type": "success",
                "title": "C√≠l splnƒõn",
                "description": "Mƒõs√≠ƒçn√≠ c√≠l na 89%",
                "icon": "Target"
            }
        ])

        return {
            "success": True,
            "data": insights,
            "meta": {
                "timestamp": datetime.now().isoformat()
            }
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get AI insights: {str(e)}")

@app.get("/api/v1/dashboard/monthly-data")
async def get_monthly_data():
    """
    Get monthly financial data for charts
    """
    try:
        # For now, return mock data - in production this would be calculated from documents
        monthly_data = [
            {"month": "Led", "income": 180000, "expenses": 120000, "profit": 60000},
            {"month": "√öno", "income": 220000, "expenses": 140000, "profit": 80000},
            {"month": "B≈ôe", "income": 190000, "expenses": 130000, "profit": 60000},
            {"month": "Dub", "income": 240000, "expenses": 150000, "profit": 90000},
            {"month": "Kvƒõ", "income": 260000, "expenses": 160000, "profit": 100000},
            {"month": "ƒåer", "income": 245000, "expenses": 156000, "profit": 89000},
        ]

        return {
            "success": True,
            "data": monthly_data,
            "meta": {
                "timestamp": datetime.now().isoformat(),
                "currency": "CZK"
            }
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get monthly data: {str(e)}")

@app.get("/api/v1/dashboard/expense-categories")
async def get_expense_categories():
    """
    Get expense categories for pie chart
    """
    try:
        # For now, return mock data - in production this would be calculated from documents
        categories = [
            {"name": "Slu≈æby", "value": 45, "color": "#3b82f6"},
            {"name": "Materi√°l", "value": 30, "color": "#10b981"},
            {"name": "Energie", "value": 15, "color": "#f59e0b"},
            {"name": "Ostatn√≠", "value": 10, "color": "#ef4444"},
        ]

        return {
            "success": True,
            "data": categories,
            "meta": {
                "timestamp": datetime.now().isoformat()
            }
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get expense categories: {str(e)}")

if __name__ == "__main__":
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8001,
        reload=False
    )
